{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdb0ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (2.32.5)\n",
      "Collecting biopython\n",
      "  Downloading biopython-1.86-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (31 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-6.33.5-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (2025.11.12)\n",
      "Collecting numpy (from biopython)\n",
      "  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.23.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Downloading fsspec-2026.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting cuda-bindings==12.9.4 (from torch)\n",
      "  Downloading cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.4.5 (from torch)\n",
      "  Downloading nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.6.0 (from torch)\n",
      "  Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting cuda-pathfinder~=1.1 (from cuda-bindings==12.9.4->torch)\n",
      "  Downloading cuda_pathfinder-1.3.4-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting huggingface-hub<2.0,>=1.3.0 (from transformers)\n",
      "  Downloading huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2026.1.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typer-slim (from transformers)\n",
      "  Downloading typer_slim-0.23.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Collecting typer>=0.23.1 (from typer-slim->transformers)\n",
      "  Downloading typer-0.23.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting click>=8.0.0 (from typer>=0.23.1->typer-slim->transformers)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting rich>=10.11.0 (from typer>=0.23.1->typer-slim->transformers)\n",
      "  Downloading rich-14.3.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from typer>=0.23.1->typer-slim->transformers)\n",
      "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer>=0.23.1->typer-slim->transformers)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->transformers) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.1->typer-slim->transformers)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading biopython-1.86-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl (915.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m  \u001b[33m0:00:23\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m  \u001b[33m0:00:15\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m  \u001b[33m0:00:19\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m  \u001b[33m0:00:09\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (139.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cuda_pathfinder-1.3.4-py3-none-any.whl (30 kB)\n",
      "Downloading transformers-5.1.0-py3-none-any.whl (10.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-1.4.1-py3-none-any.whl (553 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.33.5-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading fsspec-2026.2.0-py3-none-any.whl (202 kB)\n",
      "Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2026.1.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.23.0-py3-none-any.whl (22 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.23.1-py3-none-any.whl (3.4 kB)\n",
      "Downloading typer-0.23.1-py3-none-any.whl (56 kB)\n",
      "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading rich-14.3.2-py3-none-any.whl (309 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, tqdm, sympy, shellingham, sentencepiece, safetensors, regex, protobuf, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, mdurl, hf-xet, fsspec, filelock, cuda-pathfinder, click, annotated-doc, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, markdown-it-py, cuda-bindings, biopython, rich, nvidia-cusolver-cu12, typer, torch, typer-slim, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43/43\u001b[0m [transformers]sformers]nizers]-hub]cu12]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-doc-0.0.4 biopython-1.86 click-8.3.1 cuda-bindings-12.9.4 cuda-pathfinder-1.3.4 filelock-3.23.0 fsspec-2026.2.0 hf-xet-1.2.0 huggingface-hub-1.4.1 markdown-it-py-4.0.0 mdurl-0.1.2 mpmath-1.3.0 networkx-3.6.1 numpy-2.4.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.4.5 nvidia-nvtx-cu12-12.8.90 protobuf-6.33.5 regex-2026.1.15 rich-14.3.2 safetensors-0.7.0 sentencepiece-0.2.1 shellingham-1.5.4 sympy-1.14.0 tokenizers-0.22.2 torch-2.10.0 tqdm-4.67.3 transformers-5.1.0 triton-3.6.0 typer-0.23.1 typer-slim-0.23.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "âœ“ All libraries imported\n"
     ]
    }
   ],
   "source": [
    "%pip install requests biopython torch transformers sentencepiece protobuf\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from Bio import SeqIO\n",
    "import io\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ“ All libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d86a0d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TASK 1 COMPLETE\n",
      "========================================\n",
      "Donor ID: Donor_001\n",
      "Donor alleles: ['A*02:01', 'A*24:02', 'B*44:02', 'B*51:01']\n",
      "\n",
      "Recipient ID: Recipient_123\n",
      "Recipient alleles: ['A*01:01', 'A*02:01', 'B*44:02', 'B*08:01']\n",
      "========================================\n",
      "Total donor alleles: 4\n",
      "Total recipient alleles: 4\n"
     ]
    }
   ],
   "source": [
    "donor_profile = {\n",
    "    \"id\": \"Donor_001\",\n",
    "    \"hla_a\":[\"A*02:01\", \"A*24:02\"],\n",
    "    \"hla_b\": [\"B*44:02\", \"B*51:01\"],\n",
    "    \"hla_drb1\":[\"DRB1*04\", \"DRB*11:04\"]\n",
    "}\n",
    "recipient_profile = {\n",
    "    \"id\": \"Recipient_123\",\n",
    "    \"hla_a\": [\"A*01:01\", \"A*02:01\"],\n",
    "    \"hla_b\": [\"B*44:02\", \"B*08:01\"],\n",
    "    \"hla_drb1\": [\"DRB1*04:05\", \"DRB1*15:01\"]\n",
    "}\n",
    "\n",
    "def get_all_alleles(profile):\n",
    "  alleles =[]\n",
    "  hla_keys = ['hla_a', 'hla_b', 'hlb_drb1']\n",
    "\n",
    "  for key in hla_keys:\n",
    "    if key in profile:\n",
    "      alleles.extend(profile[key])\n",
    "\n",
    "  return alleles\n",
    "\n",
    "donor_alleles=get_all_alleles(donor_profile)\n",
    "recipient_alleles = get_all_alleles(recipient_profile)\n",
    "\n",
    "print(\"âœ… TASK 1 COMPLETE\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Donor ID: {donor_profile['id']}\")\n",
    "print(f\"Donor alleles: {donor_alleles}\")\n",
    "print()\n",
    "print(f\"Recipient ID: {recipient_profile['id']}\")\n",
    "print(f\"Recipient alleles: {recipient_alleles}\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total donor alleles: {len(donor_alleles)}\")\n",
    "print(f\"Total recipient alleles: {len(recipient_alleles)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c59407e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸš€ TASK 3: FETCHING ALL SEQUENCES\n",
      "======================================================================\n",
      "âœ… Loaded cached sequences: 44726 sequences\n",
      "\n",
      "1. Donor sequences:\n",
      "\n",
      "ðŸ“¥ Fetching 4 sequences...\n",
      "  [1/4] Fetching A*02:01: âœ… Real (365 aa)\n",
      "  [2/4] Fetching A*24:02: âœ… Real (365 aa)\n",
      "  [3/4] Fetching B*44:02: âœ… Real (362 aa)\n",
      "  [4/4] Fetching B*51:01: âœ… Real (362 aa)\n",
      "\n",
      "2. Recipient sequences:\n",
      "\n",
      "ðŸ“¥ Fetching 4 sequences...\n",
      "  [1/4] Fetching A*01:01: âœ… Real (365 aa)\n",
      "  [2/4] Fetching A*02:01: âœ… Real (365 aa)\n",
      "  [3/4] Fetching B*44:02: âœ… Real (362 aa)\n",
      "  [4/4] Fetching B*08:01: âœ… Real (362 aa)\n",
      "\n",
      "Donor summary: 4 real sequences, 0 dummy sequences.\n",
      "\n",
      "Recipient summary: 4 real sequences, 0 dummy sequences.\n",
      "âœ“ Saved to /workspaces/Donor-Recipient-Compatibility/donor_sequences.fasta\n",
      "âœ“ Saved to /workspaces/Donor-Recipient-Compatibility/recipient_sequences.fasta\n",
      "\n",
      "======================================================================\n",
      "âœ… TASK 3 COMPLETE!\n",
      "   HLA sequences fetched from cached database.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO\n",
    "import io\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import the setup script helper\n",
    "from setup_hla_database import load_hla_path\n",
    "\n",
    "class HLASequenceCache:\n",
    "    def __init__(self):\n",
    "        self.cache_dir = Path.home() / \".cache\" / \"hai_def\" if os.name != 'nt' else Path.home() / \"AppData\" / \"Local\" / \"hai_def_cache\"\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.sequence_cache_file = self.cache_dir / \"hla_sequence_cache.json\"\n",
    "        self.sequence_cache = {}\n",
    "        self._load_or_create_cache()\n",
    "\n",
    "    def _download_complete_hla_dataset(self) -> Dict:\n",
    "        \"\"\"Load HLA dataset from cached FASTA file.\"\"\"\n",
    "        # Use the cached file from setup script\n",
    "        cached_fasta_path = load_hla_path()\n",
    "        \n",
    "        if not cached_fasta_path:\n",
    "            print(\"âŒ HLA database not found in cache.\")\n",
    "            print(\"   Please run: python setup_hla_database.py\")\n",
    "            return {}\n",
    "        \n",
    "        if not cached_fasta_path.exists():\n",
    "            print(f\"âŒ Cached FASTA file not found at: {cached_fasta_path}\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"ðŸ“¥ Loading HLA FASTA database from cache: {cached_fasta_path}\")\n",
    "        try:\n",
    "            with open(cached_fasta_path, 'r') as f:\n",
    "                fasta_content = f.read()\n",
    "            dataset = self._parse_fasta_to_dataset(fasta_content)\n",
    "            if dataset:\n",
    "                print(f\"âœ… Parsed {len(dataset)} unique HLA protein sequences from cached file\")\n",
    "                return dataset\n",
    "            else:\n",
    "                print(\"âŒ Failed to parse FASTA content.\")\n",
    "                return {}\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error reading cached FASTA file: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def _parse_fasta_to_dataset(self, fasta_content: str) -> Dict:\n",
    "        \"\"\"Parse FASTA content into dataset, storing the longest sequence for broader allele groups.\"\"\"\n",
    "        print(\"  Parsing FASTA file...\")\n",
    "\n",
    "        dataset = {}\n",
    "        fasta_io = io.StringIO(fasta_content)\n",
    "\n",
    "        for record in SeqIO.parse(fasta_io, \"fasta\"):\n",
    "            header = record.description\n",
    "            sequence = str(record.seq)\n",
    "\n",
    "            # Skip if sequence is too short to be considered 'real'\n",
    "            if len(sequence) < 100:\n",
    "                continue\n",
    "\n",
    "            allele_name = self._extract_allele_name(header)\n",
    "\n",
    "            if allele_name:\n",
    "                # Store the full allele name and its sequence\n",
    "                if allele_name not in dataset or len(sequence) > len(dataset[allele_name]):\n",
    "                    dataset[allele_name] = sequence\n",
    "\n",
    "                # Handle broader allele groups (e.g., DRB1*04:01, DRB1*04)\n",
    "                if '*' in allele_name:\n",
    "                    parts = allele_name.split('*')\n",
    "                    locus_prefix = parts[0]\n",
    "                    allele_fields = parts[1].split(':')\n",
    "\n",
    "                    # Store 3-field version (e.g., A*02:01:01)\n",
    "                    if len(allele_fields) >= 3:\n",
    "                        simplified_3_field = f\"{locus_prefix}*{allele_fields[0]}:{allele_fields[1]}:{allele_fields[2]}\"\n",
    "                        if simplified_3_field not in dataset or len(sequence) > len(dataset[simplified_3_field]):\n",
    "                            dataset[simplified_3_field] = sequence\n",
    "\n",
    "                    # Store 2-field version (e.g., A*02:01)\n",
    "                    if len(allele_fields) >= 2:\n",
    "                        simplified_2_field = f\"{locus_prefix}*{allele_fields[0]}:{allele_fields[1]}\"\n",
    "                        if simplified_2_field not in dataset or len(sequence) > len(dataset[simplified_2_field]):\n",
    "                            dataset[simplified_2_field] = sequence\n",
    "\n",
    "                    # Store 1-field version (e.g., A*02)\n",
    "                    simplified_1_field = f\"{locus_prefix}*{allele_fields[0]}\"\n",
    "                    if simplified_1_field not in dataset or len(sequence) > len(dataset[simplified_1_field]):\n",
    "                        dataset[simplified_1_field] = sequence\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def _extract_allele_name(self, header: str) -> Optional[str]:\n",
    "        \"\"\"Extract HLA allele name from FASTA header using a robust regex.\"\"\"\n",
    "        allele_name_pattern = r'([A-Z0-9]+\\*\\d+(?::\\d+){0,3})'\n",
    "\n",
    "        match = re.search(allele_name_pattern, header)\n",
    "\n",
    "        if match:\n",
    "            extracted_allele = match.group(1)\n",
    "            if '*' in extracted_allele and re.match(r'^[A-Z0-9]+\\*\\d+(?::\\d+){0,3}$', extracted_allele):\n",
    "                return extracted_allele\n",
    "        return None\n",
    "\n",
    "    def _build_sequence_cache(self, dataset: Dict) -> Dict:\n",
    "        \"\"\"Build optimized sequence cache.\"\"\"\n",
    "        print(\"ðŸ”¨ Building optimized sequence cache...\")\n",
    "\n",
    "        sequence_cache = {}\n",
    "\n",
    "        for allele_name, sequence in dataset.items():\n",
    "            if sequence and len(sequence) > 100:\n",
    "                sequence_cache[allele_name] = sequence\n",
    "\n",
    "        print(f\"âœ… Cache built with {len(sequence_cache)} sequences\")\n",
    "        return sequence_cache\n",
    "\n",
    "    def _load_or_create_cache(self):\n",
    "        \"\"\"Load cache or create new.\"\"\"\n",
    "        if self.sequence_cache_file.exists():\n",
    "            try:\n",
    "                with open(self.sequence_cache_file, 'r') as f:\n",
    "                    self.sequence_cache = json.load(f)\n",
    "                print(f\"âœ… Loaded cached sequences: {len(self.sequence_cache)} sequences\")\n",
    "                return\n",
    "            except:\n",
    "                print(\"âš ï¸ Cache corrupted, rebuilding...\")\n",
    "\n",
    "        print(\"ðŸ“¥ Building sequence cache from FASTA...\")\n",
    "        dataset = self._download_complete_hla_dataset()\n",
    "        if dataset:\n",
    "            self.sequence_cache = self._build_sequence_cache(dataset)\n",
    "            if self.sequence_cache:\n",
    "                with open(self.sequence_cache_file, 'w') as f:\n",
    "                    json.dump(self.sequence_cache, f, indent=2)\n",
    "                print(f\"ðŸ’¾ Saved sequence cache to {self.sequence_cache_file}\")\n",
    "\n",
    "    def get_sequence(self, allele_name: str) -> str:\n",
    "        \"\"\"Get sequence with multiple fallback strategies.\"\"\"\n",
    "        if allele_name in self.sequence_cache:\n",
    "            seq = self.sequence_cache[allele_name]\n",
    "            if len(seq) > 100:\n",
    "                return seq\n",
    "\n",
    "        api_seq = self._fetch_from_api(allele_name)\n",
    "        if api_seq and len(api_seq) > 100:\n",
    "            self.sequence_cache[allele_name] = api_seq\n",
    "            self.save_cache()\n",
    "            return api_seq\n",
    "\n",
    "        return self._get_dummy_sequence(allele_name)\n",
    "\n",
    "    def _fetch_from_api(self, allele_name: str) -> Optional[str]:\n",
    "        \"\"\"API fallback for individual alleles.\"\"\"\n",
    "        encoded_allele = allele_name.replace(':', '%3A')\n",
    "\n",
    "        try:\n",
    "            search_url = f\"https://www.ebi.ac.uk/cgi-bin/ipd/api/allele?name={encoded_allele}&release=latest\"\n",
    "            search_response = requests.get(search_url, timeout=15)\n",
    "\n",
    "            if search_response.status_code != 200:\n",
    "                return None\n",
    "\n",
    "            search_data = search_response.json()\n",
    "\n",
    "            accession = None\n",
    "            if isinstance(search_data, dict) and 'data' in search_data and isinstance(search_data['data'], list) and len(search_data['data']) > 0:\n",
    "                for item in search_data['data']:\n",
    "                    item_name = item.get('name', '')\n",
    "                    if item_name.startswith(allele_name):\n",
    "                        accession = item.get('accession')\n",
    "                        break\n",
    "\n",
    "            if not accession:\n",
    "                return None\n",
    "\n",
    "            allele_detail_url = f\"https://www.ebi.ac.uk/cgi-bin/ipd/api/allele/{accession}\"\n",
    "            detail_response = requests.get(allele_detail_url, timeout=15)\n",
    "\n",
    "            if detail_response.status_code == 200:\n",
    "                detail_data = detail_response.json()\n",
    "\n",
    "                if (isinstance(detail_data, dict) and\n",
    "                    'sequence' in detail_data and\n",
    "                    'protein' in detail_data['sequence']):\n",
    "\n",
    "                    protein_seq = detail_data['sequence']['protein']\n",
    "\n",
    "                    if protein_seq and len(protein_seq) > 100:\n",
    "                        return protein_seq\n",
    "\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass\n",
    "        except (json.JSONDecodeError, KeyError, IndexError, ValueError):\n",
    "            pass\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _get_dummy_sequence(self, allele_name: str) -> str:\n",
    "        \"\"\"Get realistic dummy sequence.\"\"\"\n",
    "        dummy_sequences = {\n",
    "            'A': \"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHF_DUMMY_SEQ\",\n",
    "            'B': \"MVHWTAEEKQLITGLWGKVNVAECGAEALARLLIVYPWTQRFFASF_DUMMY_SEQ\",\n",
    "            'DRB1': \"MVCLKLPGGSYMAKLTVQIPSSVFAVTSILMRSSAVLGLLLLGAG_DUMMY_SEQ\",\n",
    "        }\n",
    "\n",
    "        if allele_name.startswith('A'):\n",
    "            return dummy_sequences['A']\n",
    "        elif allele_name.startswith('B'):\n",
    "            return dummy_sequences['B']\n",
    "        elif allele_name.startswith('DRB1'):\n",
    "            return dummy_sequences['DRB1']\n",
    "        else:\n",
    "            return \"UNKNOWN_LOCUS_DUMMY_SEQ\"\n",
    "\n",
    "    def save_cache(self):\n",
    "        \"\"\"Save cache to disk.\"\"\"\n",
    "        try:\n",
    "            with open(self.sequence_cache_file, 'w') as f:\n",
    "                json.dump(self.sequence_cache, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving cache: {e}\")\n",
    "\n",
    "# ============================================\n",
    "# TASK 3: FETCH ALL SEQUENCES\n",
    "# ============================================\n",
    "\n",
    "def fetch_all_sequences(allele_list: List[str], hla_cache_instance) -> Dict[str, str]:\n",
    "    \"\"\"Fetch sequences using cache.\"\"\"\n",
    "    sequences = {}\n",
    "\n",
    "    print(f\"\\nðŸ“¥ Fetching {len(allele_list)} sequences...\")\n",
    "\n",
    "    for i, allele in enumerate(allele_list, 1):\n",
    "        print(f\"  [{i}/{len(allele_list)}] Fetching {allele}\", end=\": \")\n",
    "        seq = hla_cache_instance.get_sequence(allele)\n",
    "\n",
    "        if seq and len(seq) > 100: # Check for real sequence based on length\n",
    "            sequences[allele] = seq\n",
    "            print(f\"âœ… Real ({len(seq)} aa)\")\n",
    "        else:\n",
    "            sequences[allele] = seq # Store dummy too, but mark as failed\n",
    "            print(f\"âŒ Dummy ({len(seq)} aa)\")\n",
    "\n",
    "    return sequences\n",
    "\n",
    "# Your alleles from Task 1 (re-define for execution safety if not in scope)\n",
    "if 'donor_alleles' not in locals() or 'recipient_alleles' not in locals():\n",
    "    donor_profile = {\n",
    "        \"id\": \"Donor_001\",\n",
    "        \"hla_a\":[\"A*02:01\", \"A*24:02\"],\n",
    "        \"hla_b\": [\"B*44:02\", \"B*51:01\"],\n",
    "        \"hla_drb1\":[\"DRB1*04\", \"DRB1*11:04\"]\n",
    "    }\n",
    "    recipient_profile = {\n",
    "        \"id\": \"Recipient_123\",\n",
    "        \"hla_a\": [\"A*01:01\", \"A*02:01\"],\n",
    "        \"hla_b\": [\"B*44:02\", \"B*08:01\"],\n",
    "        \"hla_drb1\": [\"DRB1*04:05\", \"DRB1*15:01\"]\n",
    "    }\n",
    "\n",
    "    def get_all_alleles(profile):\n",
    "        alleles =[]\n",
    "        hla_keys = ['hla_a', 'hla_b', 'hla_drb1']\n",
    "        for key in hla_keys:\n",
    "            if key in profile:\n",
    "                alleles.extend(profile[key])\n",
    "        return alleles\n",
    "\n",
    "    donor_alleles=get_all_alleles(donor_profile)\n",
    "    recipient_alleles = get_all_alleles(recipient_profile)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸš€ TASK 3: FETCHING ALL SEQUENCES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "hla_cache = HLASequenceCache()\n",
    "\n",
    "print(\"\\n1. Donor sequences:\")\n",
    "donor_sequences = fetch_all_sequences(donor_alleles, hla_cache)\n",
    "\n",
    "print(\"\\n2. Recipient sequences:\")\n",
    "recipient_sequences = fetch_all_sequences(recipient_alleles, hla_cache)\n",
    "\n",
    "# Summary\n",
    "def analyze_sequences(seq_dict, name):\n",
    "    real_count = sum(1 for s in seq_dict.values() if len(s) > 100)\n",
    "    dummy_count = len(seq_dict) - real_count\n",
    "    print(f\"\\n{name} summary: {real_count} real sequences, {dummy_count} dummy sequences.\")\n",
    "\n",
    "analyze_sequences(donor_sequences, \"Donor\")\n",
    "analyze_sequences(recipient_sequences, \"Recipient\")\n",
    "\n",
    "# Save sequences to files\n",
    "def save_sequences_to_fasta(sequences_dict: Dict[str, str], filename_base: str):\n",
    "    \"\"\"Save sequences in FASTA format.\"\"\"\n",
    "    filepath = Path.cwd() / filename_base\n",
    "    with open(filepath, 'w') as f:\n",
    "        for allele, sequence in sequences_dict.items():\n",
    "            f.write(f\">{allele}\\n\")\n",
    "            for i in range(0, len(sequence), 60):\n",
    "                f.write(sequence[i:i+60] + \"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    print(f\"âœ“ Saved to {filepath}\")\n",
    "\n",
    "save_sequences_to_fasta(donor_sequences, \"donor_sequences.fasta\")\n",
    "save_sequences_to_fasta(recipient_sequences, \"recipient_sequences.fasta\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… TASK 3 COMPLETE!\")\n",
    "print(\"   HLA sequences fetched from cached database.\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
